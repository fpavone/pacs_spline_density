\chapter{Description of the problem to solve}
\newtheorem*{definition}{Definition}
\label{problem}
% \thispagestyle{empty}

\noindent 
\section{Introduction}
In the analysis of large-scale database systems, information is frequently summarized using a density function, that is, Borel measurable, positive function on a support $\text{I}$ with a unit integral constraint. 
% In this way, the intrinsic variability in the data is preserved and statistical analysis can be done according to the already existing methodologies. \\

Even though it might seem that density functions are just a special case of functional data, standard FDA methods appear to be inappropriate for their treatment, as they do not consider the particular constrained nature of the data. 
This problem is well known in the finite dimensional setting, where specific techniques have been worked out to deal with compositional data, i.e., multivariate data carrying only relative information, usually represented in proportions or percentages. 
Those techniques are mainly based on a geometric perspective grounded on the Aitchison geometry in the simplex, which properly incorporates the compositional nature of the data (see the 'old but gold' pioneer \citep{aitchinson:bayes}). 

In this context, probability density functions have recently been interpreted as functional compositional data, i.e., functional data carrying only relative information.
To handle this kind of data, the Aitchison geometry has been recently extended to the so called Bayes spaces: a Hilbert space structure for $\sigma$-finite measures, including probability measures, has been worked out, as in \citep{vdboogaart:bayes}.

In order to resort to standard statistical analysis of density functions, a mapping from the Bayes space to the standard $\textit{L}^2$ space is needed. 
% Hence the general idea to tackle this problem is to to express the data in the $\textit{L}^2$ space  and to perform the computations there.

An isometric isomorphism between the Bayes space and the Hilbert space $\textit{L}^2$ of (equivalence classes of) square-integrable real functions on the support $\text{I}$ is defined by the centred log-ratio (clr) transformation, explicitly 
% It follows from the property that the logarithm of probability density functions is square-integrable (**[11]**): considering the finite interval support case I, where Lebesgue measure is used, the simplest isomorphism is the centred logratio (clr) transformation, defined as 
\[	clr[f(x)] = f_c(x) = ln(f(x)) - \frac{1}{\eta}\int_{I} ln(f(x))\, dx 	\]
where $f(x)$ is a density function, $\eta$ is the length of the interval I.

The antitranformation is defined as: 
\[  clr^{-1}[f(x)]= \frac{exp(f_c(x))}{\int_{I} f_c(x)\, dx}. 	\]
It's immediate to notice that the following constraint holds:
\[	\int_{I} clr[f(x)]\, dx = 0. 			\]
This additional condition needs to be taken into account for computation and analysis on clr-transformed density functions.

%%% NOTE (hope to be rigth)
% Remember what dr. Menafoglio told us about the problem: starting from discretized data, we could smooth and then transform them, in order to use them alongside with the unit integral constraint. But it would be uneasy to deal with a constraint on the b-spline coeffients. 
% Therefore, what we do is transform and and smooth in such a way that the constraint is automatically satisfied. It requires a little bit more of calculus but that's a big advantage

\section{Why B-spline}
In practice the aggregation of individual observations to a discretized form of histogram data is done and a smoothed version of the distribution has to be found.
Spline functions are often chosen for the approximation of non-periodic functional data since they allow fast computation and flexibility. The most famous system of spline for this kind of problem is the smoothing B-spline basis system developed by de Boor (\cite{ramsay:FDA}). This is because the resulting coefficients of basis functions can be directly used for statistical analysis. %2005_Book_FunctionalDataAnalysis

\section{B-spline representation}
Let $\Delta\lambda := \lambda_0=u < \lambda_1< \dots < \lambda_g < v = \lambda_{g+1} $ be a sequence of knots, $S_k^{\Delta\lambda}[u,v]$ the vector space of polynomial splines of degree $k > 0$ defined on the interval $[u,v]$ using $\Delta\lambda$, $dim(S_k^{\Delta\lambda}[u,v]) = g + k + 1$. \\
A spline in this space has the following form:
\[  s_k(\textbf{x})=\sum\limits_{i=-k}^{g}b_iB_i^{k+1}(\textbf{x}) \]
where ${B_i^{k+1}}$ are B-splines of degree k and form a basis in $S_k^{\Delta\lambda}[u,v]$. For this representation additional knots are required:
\[  \lambda_{-k}= \dots = \lambda_{-1} = \lambda_0,  \ \ \ \ \lambda_{g+1}= \dots = \lambda_{g+k+1}. \]
%%% NOTE: maybe we can skip the upper part
In matrix notation, the spline can be rewritten as:
\[  s_k(\textbf{x})=  \textbf{C}_{k+1}(\textbf{x})\textbf{b}, \]
where $ \textbf{C}_{k+1}(\textbf{x})$ is the \textit{collocation matrix}, defined as
\[  \textbf{C}_{k+1}(\textbf{x}) =
\begin{bmatrix}
B_{-k}^{k+1}(x_1)  & \dots  & B_{g}^{k+1}(x_1) \\
\vdots & \ddots & \vdots \\
B_{-k}^{k+1}(x_n) &  \dots  & B_{g}^{k+1}(x_n)
\end{bmatrix} \in \mathbb{R}^{n, g+k+1} \]
and  $\textbf{b}= [ b_{-k}, \dots, b_{g}]^T$ is \textit{the vector of B-spline coefficients of} $s_k(\textbf{x})$.\\
Moreover, using properties of B-splines, it is possible to write the derivative of order $l$ of the spline $s_k(\textbf{x})$ as
\[  s_k^{(l)}(\textbf{x})=  \textbf{C}_{k+1-l}(\textbf{x})\textbf{b}^{(l)}, \]
where $\textbf{b}^{(l)}$ can be computed recursively:
\[ \textbf{b}^{(l)} = \textbf{D}_l \textbf{L}_l \textbf{b}^{(l-1)}   =  \textbf{D}_l \textbf{L}_l \dots \textbf{D}_1 \textbf{L}_1\textbf{b} = \textbf{S}_l\textbf{b}, \]
\[ \textbf{D}_j =  (k+1-j) \text{diag}(d_{-k+j},\dots, d_g)    \]
\[ \text{with} \ d_i = \frac{1}{\lambda_{i+k+1-j}-\lambda_i}   \forall i = -k+j,\dots, g,  \]
\[ \textbf{L}_j =  \begin{bmatrix}
-1  & 1 &  &\\
 & \ddots & \ddots&\\
 & & -1&1
\end{bmatrix}  \in \mathbb{R}^{g+k+1-j,g+k+2-j}.\]

\section{The optimal smoothing problem} \label{optimal}
Our goal is to find a function $f(x)$ that is a smooth approximation, close enough to the given data points $(x_i,y_i)$. Hence $f(x)$ has to fulfil the following optimization problem:
\begin{equation*} 
\begin{aligned}
& \underset{f}{\text{minimize}}
& & \int_{x_1}^{x_n} [f^{(l)}(x)]^2 \,dx \\
& \text{subject to}
& & \sum\limits_{i=1}^{n}[w_i(y_i-f(x_i))]^2 \le S
\end{aligned}
\end{equation*}
where the objective function is a measure of non-smoothness of $f(x)$ and the constraint is a measure of the closeness of fit that takes into account data accuracy using weights.
The solution is known to be a natural spline $s_k(\textbf{x})$ of degree $k=2l-1$.
It follows that for $l \ge 2$  
\[s_k^{(l+j)}(x_1)=s_k^{(l+j)}(x_n)=0, \ \ \ j=0,1,\dots,l-2.\]
In order to find $\textbf{b}$, we can plug this result into the dual problem:
\begin{equation*} 
\begin{aligned}
& \underset{f}{\text{minimize}}
& &  J_l(f) \\
\end{aligned}
\end{equation*}
$\text{where} \ J_l(f) :=\int_{x_1}^{x_n} [f^{(l)}(x)]^2 \,dx + \alpha \sum\limits_{i=1}^{n}[w_i(y_i-f(x_i))]^2$.\\
Assuming data $(x_i,y_i), \ u\le x_i \le v$, $\textbf{x} = [x_1, \dots, x_n]^\top$, $\textbf{y} = [y_1, \dots, y_n]^\top$ weights $w_i \ge 0, \ i=1, \dots, n$, sequence of knots $\Delta\lambda$, $n \ge g+1 $ and $\alpha \in (0,1)$ are given, % If $\alpha is not given, find it throgh crossvalidation 
and rewriting the functional $J_l(f)$ in matrix notation, we find:
\[J_l(\textbf{b}) = \textbf{b}^\top \textbf{N}_{kl}\textbf{b} + \alpha [\textbf{y}-\textbf{C}_{k+1}(\textbf{x})\textbf{b}]^\top \textbf{W} [\textbf{y}-\textbf{C}_{k+1}(\textbf{x})\textbf{b}]\]
where $\textbf{N}_{kl} =  \textbf{S}_l^\top \textbf{M}_{kl}\textbf{S}_l$ is positive semidefinite, 
\[  \textbf{M}_{kl} =
\begin{bmatrix}
(B_{-k+l}^{k+1-l}, B_{-k+l}^{k+1-l}) & \dots  & (B_{g}^{k+1-l}, B_{-k+l}^{k+1-l}) \\
\vdots & \ddots & \vdots \\
(B_{-k+l}^{k+1-l}, B_{g}^{k+1-l}) &  \dots  & (B_{g}^{k+1-l}, B_{g}^{k+1-l})
\end{bmatrix} \in \mathbb{R}^{g+k+1-l, g+k+1-l} \]
and \[(B_{i}^{k+1-l}, B_{j}^{k+1-l}) = \int_u^v B_{i}^{k+1-l}(x)B_{j}^{k+1-l}(x) \, dx.\]
$\textbf{M}_{kl}$ is positive definite because each element is the scalar product in $L^2([u,v])$ of basis functions. \\
In the case of  smoothed clr-transformed density functions, we have to add the condition
\[\int_u^v s_k(x) \, dx = 0.\] 
From B-spline properties we know that 
\[  s_k(\textbf{x})=\sum\limits_{i=-k}^{g}b_iB_i^{k+1}(\textbf{x}) \]
is the derivative of the spline
\[  s_{k+1}(\textbf{x})=\sum\limits_{i=-k-1}^{g}c_iB_i^{k+2}(\textbf{x}) \]
if
\[b_i = (k+1)\frac{c_i-c_{i-1}}{\lambda_{i+k+1}-\lambda_{i}}\ \ \  \forall i = -k, \dots, g.\]
Hence 
\[ 0 = \int_u^v s_k(x) \, dx = [s_{k+1}(x)]_u^v = s_{k+1}(\lambda_{g+1})-s_{k+1}(\lambda_{0}) = c_g-c_{-k-1},\]
so that $c_g = c_{-k-1}$.\\
Finally we have identified a relationship between $\textbf{b}$ and $\textbf{c}$:
\[\textbf{b} = \textbf{D}\textbf{K}\textbf{c}\] 
where $ \textbf{D} = \textbf{D}_0$ and
\[  \textbf{K} =
\begin{bmatrix}
1 &0&0& \dots  & -1 \\
-1 &1&0& \dots  & 0 \\
0 &-1&1& \dots  & 0 \\
\vdots &\vdots & \ddots & \ddots& \vdots \\
0 &0 & \dots&-1  & 1
\end{bmatrix} \in \mathbb{R}^{g+k+1, g+k+1}. \]
Hence the functional to minimize can be rewritten depending on $\textbf{c}$
\[J_l(\textbf{c}) = (\textbf{D}\textbf{K}\textbf{c})^\top \textbf{N}_{kl}\textbf{D}\textbf{K}\textbf{c} + \alpha [\textbf{y}-\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K}\textbf{c}]^\top \textbf{W} [\textbf{y}-\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K}\textbf{c}]\]
In order to find the minimum, we set the derivative of $J_l(\textbf{c})$ w.r.t. $\textbf{c}$ to 0 e we obtain
\[ [\alpha^{-1}(\textbf{D}\textbf{K})^\top \textbf{N}_{kl} \textbf{D}\textbf{K}+ (\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K})^\top\textbf{W}\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K}]\textbf{c} = (\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K})^\top \textbf{W}\textbf{y}.\]
If $\alpha^{-1}(\textbf{D}\textbf{K})^\top \textbf{N}_{kl} \textbf{D}\textbf{K}+ (\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K})^\top\textbf{W}\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K}$ is invertible, then there exists a unique solution 
\[\textbf{c}^\star = [\alpha^{-1}(\textbf{D}\textbf{K})^\top \textbf{N}_{kl} \textbf{D}\textbf{K}+ (\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K})^\top\textbf{W}\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K}]^{-1}\textbf{K}^\top\textbf{D}^\top\textbf{C}_{k+1}^\top \textbf{W}\textbf{y},\]
otherwise we can find a minimum norm solution
\[\textbf{c}^\star = [\alpha^{-1}(\textbf{D}\textbf{K})^\top \textbf{N}_{kl} \textbf{D}\textbf{K}+ (\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K})^\top\textbf{W}\textbf{C}_{k+1}(\textbf{x})\textbf{D}\textbf{K}]_{m}^{-}\textbf{K}^\top\textbf{D}^\top\textbf{C}_{k+1}^\top \textbf{W}\textbf{y}.\]
In both cases the vector of B-spline coefficients is obtained by
\[\textbf{b}^\star = \textbf{D}\textbf{K}\textbf{c}^\star.\] 


